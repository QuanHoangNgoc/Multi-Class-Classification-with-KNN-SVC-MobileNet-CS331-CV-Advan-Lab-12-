{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2jcf-XN3487A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUVI1UJw3reA",
    "outputId": "f3329020-76b3-44d0-e976-fbfb40d4caa0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\n",
      "From (redirected): https://drive.usercontent.google.com/download?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp&confirm=t&uuid=f1720424-9f93-46fd-87f9-b65544b0f254\n",
      "To: /content/caltech101/101_ObjectCategories.tar.gz\n",
      "100%|██████████| 132M/132M [00:00<00:00, 174MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./caltech101/101_ObjectCategories.tar.gz to ./caltech101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=175kQy3UsZ0wUEHZjqkUDdNVssr7bgh_m\n",
      "From (redirected): https://drive.usercontent.google.com/download?id=175kQy3UsZ0wUEHZjqkUDdNVssr7bgh_m&confirm=t&uuid=4b621762-4135-406c-9c1c-bb9cbeb64c83\n",
      "To: /content/caltech101/Annotations.tar\n",
      "100%|██████████| 14.0M/14.0M [00:00<00:00, 220MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./caltech101/Annotations.tar to ./caltech101\n"
     ]
    }
   ],
   "source": [
    "data_path= './'\n",
    "dataset =  datasets.Caltech101(data_path, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zi8jqM8Vac4O",
    "outputId": "45f21b93-72c8-4923-c525-c7024ba2e22d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/caltech101/101_ObjectCategories/dollar_bill/image_0010.jpg', '/content/caltech101/101_ObjectCategories/dollar_bill/image_0037.jpg', '/content/caltech101/101_ObjectCategories/dollar_bill/image_0039.jpg', '/content/caltech101/101_ObjectCategories/dollar_bill/image_0020.jpg', '/content/caltech101/101_ObjectCategories/dollar_bill/image_0051.jpg']\n"
     ]
    }
   ],
   "source": [
    "# prompt: List all image path whose label is not BACKGROUND_Google\n",
    "\n",
    "image_paths = list(paths.list_images('/content/caltech101/101_ObjectCategories'))\n",
    "print(image_paths[:5])\n",
    "image_paths_filtered = [path for path in image_paths if 'BACKGROUND_Google' not in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Dh81YinIIInY"
   },
   "outputs": [],
   "source": [
    "def get_image(path, transform=None):\n",
    "  img = cv2.imread(path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  if transform is not None:\n",
    "    img = transform(img)\n",
    "\n",
    "  return img\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dL2v236DK_gt",
    "outputId": "40c3ef02-c7e6-4edd-90da-cfc574127f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: ['dollar_bill', 'dollar_bill', 'dollar_bill', 'dollar_bill', 'dollar_bill']\n",
      "Encoded labels: [32 32 32 32 32]\n",
      "One-hot encoded labels:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# prompt: read the label and apply one-hot encoder\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Assuming you have a list of labels called 'labels'\n",
    "labels = [path.split(os.path.sep)[-2] for path in image_paths_filtered]\n",
    "\n",
    "# Initialize the label encoder\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "\n",
    "# Initialize the one-hot encoder\n",
    "lb = LabelBinarizer()\n",
    "labels_one_hot = lb.fit_transform(labels_encoded)\n",
    "\n",
    "# Print some examples\n",
    "print(\"Original labels:\", labels[:5])\n",
    "print(\"Encoded labels:\", labels_encoded[:5])\n",
    "print(\"One-hot encoded labels:\\n\", labels_one_hot[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhyXLikl7xGo",
    "outputId": "edf56bda-5598-4460-d354-95a4543af74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5206\n",
      "1735\n",
      "1736\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.6\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(image_paths_filtered, labels_one_hot, test_size=1 - train_ratio)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_val))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-dZdPCPT63x",
    "outputId": "94086589-d010-4d92-df5b-3be09c01fd24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mn8mdb6-SsLS"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        if len(x) != len(y):\n",
    "            print('Cannot create dataset: x and y have different length')\n",
    "            return\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = get_image(self.x[idx], transform)\n",
    "        y = torch.tensor(self.y[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xzWRw1zzTjGa"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(x_train, y_train)\n",
    "val_dataset = CustomDataset(x_val, y_val)\n",
    "test_dataset = CustomDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qQlNEa_dTpLA"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_tcEI1P9_JWE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1) # (32, 112, 112)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.dwconv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, groups=32) # (32, 112, 112)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=1, stride=1, padding=0) # (64, 112, 112)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.dwconv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, groups=64) # (64, 56, 56)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0) # (128, 56, 56)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.dwconv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, groups=128) # (128, 56, 56)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0) # (128, 56, 56)\n",
    "        self.bn7 = nn.BatchNorm2d(128)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        self.dwconv4 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, groups=128) # (128, 28, 28)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        self.relu8 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0) # (256, 28, 28)\n",
    "        self.bn9 = nn.BatchNorm2d(256)\n",
    "        self.relu9 = nn.ReLU()\n",
    "\n",
    "        self.dwconv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, groups=256) # (256, 28, 28)\n",
    "        self.bn10 = nn.BatchNorm2d(256)\n",
    "        self.relu10 = nn.ReLU()\n",
    "\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0) # (256, 28, 28)\n",
    "        self.bn11 = nn.BatchNorm2d(256)\n",
    "        self.relu11 = nn.ReLU()\n",
    "\n",
    "        self.dwconv6 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, groups=256) # (256, 14, 14)\n",
    "        self.bn12 = nn.BatchNorm2d(256)\n",
    "        self.relu12 = nn.ReLU()\n",
    "\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=1, stride=1, padding=0) # (512, 14, 14)\n",
    "        self.bn13 = nn.BatchNorm2d(512)\n",
    "        self.relu13 = nn.ReLU()\n",
    "\n",
    "        # Repeat blocks for depthwise + pointwise convolutions 5 times\n",
    "        self.dwconv_repeats = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, groups=512),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(512, 512, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            for _ in range(5)\n",
    "        ])\n",
    "\n",
    "        self.dwconv12 = nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1, groups=512) # (512, 7, 7)\n",
    "        self.bn24 = nn.BatchNorm2d(512)\n",
    "        self.relu24 = nn.ReLU()\n",
    "\n",
    "        self.conv13 = nn.Conv2d(512, 1024, kernel_size=1, stride=1, padding=0) # (1024, 7, 7)\n",
    "        self.bn25 = nn.BatchNorm2d(1024)\n",
    "        self.relu25 = nn.ReLU()\n",
    "\n",
    "        self.dwconv13 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1, groups=1024) # (1024, 7, 7)\n",
    "        self.bn26 = nn.BatchNorm2d(1024)\n",
    "        self.relu26 = nn.ReLU()\n",
    "\n",
    "        self.conv14 = nn.Conv2d(1024, 1024, kernel_size=1, stride=1, padding=0) # (1024, 7, 7)\n",
    "        self.bn27 = nn.BatchNorm2d(1024)\n",
    "        self.relu27 = nn.ReLU()\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # (1024, 1, 1)\n",
    "        self.fc = nn.Linear(1024, num_classes) # (num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu2(self.bn2(self.dwconv1(x)))\n",
    "        x = self.relu3(self.bn3(self.conv2(x)))\n",
    "        x = self.relu4(self.bn4(self.dwconv2(x)))\n",
    "        x = self.relu5(self.bn5(self.conv3(x)))\n",
    "        x = self.relu6(self.bn6(self.dwconv3(x)))\n",
    "        x = self.relu7(self.bn7(self.conv4(x)))\n",
    "        x = self.relu8(self.bn8(self.dwconv4(x)))\n",
    "        x = self.relu9(self.bn9(self.conv5(x)))\n",
    "        x = self.relu10(self.bn10(self.dwconv5(x)))\n",
    "        x = self.relu11(self.bn11(self.conv6(x)))\n",
    "        x = self.relu12(self.bn12(self.dwconv6(x)))\n",
    "        x = self.relu13(self.bn13(self.conv7(x)))\n",
    "\n",
    "        x = self.dwconv_repeats(x)\n",
    "\n",
    "        x = self.relu24(self.bn24(self.dwconv12(x)))\n",
    "        x = self.relu25(self.bn25(self.conv13(x)))\n",
    "        x = self.relu26(self.bn26(self.dwconv13(x)))\n",
    "        x = self.relu27(self.bn27(self.conv14(x)))\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lLEkDraPgwfE"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vjCGSrEnHLAD"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bP--gtUA9iq1"
   },
   "source": [
    "### Yêu cầu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGXv4gaU9TJh",
    "outputId": "f8105d87-6a1b-4495-872d-e237a6a694e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:34<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.095532123635455\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:32<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 3.7311997064730016\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 3.4168789968257998\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:32<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 3.1695559751696702\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 2.991819152017919\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 2.8198303274992034\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:32<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 2.681969380960232\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:33<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 2.543353333705809\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 2.402984610417994\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 2.273132894097305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:23<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 51.383019592777565%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:07<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 39.711815561959654%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:07<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 40.89861751152074%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(lb.classes_)\n",
    "model = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch '+str(epoch+1)+'/'+str(num_epochs))\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Train Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, total=len(val_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, total=len(test_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfMVuN9GkGG8",
    "outputId": "26c040fe-95dc-499c-a8ec-1c3f7de5d733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:33<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.660675915276132\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 4.638066466261701\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 4.614502278769889\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:32<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 4.592762749369552\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:33<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 4.569505394958869\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 4.549060879684076\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:32<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 4.528073857470257\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:33<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 4.510868049249416\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:32<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 4.491658065377212\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 4.472105997364696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:21<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 9.335382251248559%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:07<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 9.337175792507205%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:06<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 9.735023041474655%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(lb.classes_)\n",
    "model = CNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch '+str(epoch+1)+'/'+str(num_epochs))\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Train Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, total=len(val_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, total=len(test_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "i8MyidLbdjH9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, float):\n",
    "                focal_loss *= self.alpha\n",
    "            elif isinstance(self.alpha, torch.Tensor):\n",
    "                alpha_t = self.alpha[targets]\n",
    "                focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "016RdFt0ox_U",
    "outputId": "2a4ace94-8440-43c4-b0f2-2627de29ceaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:34<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3.9536373353585965\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:34<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 3.5060522294626004\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:33<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 3.2131332944079145\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:34<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 2.956688485494474\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:34<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 2.7610703445062406\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:33<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 2.588824417532944\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:34<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 2.4238397784349397\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:34<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 2.2770482839607613\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:33<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 2.129527608068978\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:34<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 1.972097372136465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:21<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 51.575105647330005%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:06<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 38.674351585014406%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:07<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 39.80414746543779%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(lb.classes_)\n",
    "model = CNN(num_classes)\n",
    "criterion = FocalLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch '+str(epoch+1)+'/'+str(num_epochs))\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Train Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, total=len(val_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, total=len(test_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhTwuQUno79p",
    "outputId": "00e0f763-3d97-4d1c-d3fc-dc655c6096c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.511571802744052\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 4.487471568875197\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 4.46093296423191\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 4.438488884669979\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 4.414338204918838\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 4.391597410527671\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 4.368652931073817\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 4.347016863706635\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 4.326468421191704\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 4.306427618352378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:20<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 13.887821744141375%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:07<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 12.507204610951009%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:06<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 13.248847926267281%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(lb.classes_)\n",
    "model = CNN(num_classes)\n",
    "criterion = FocalLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch '+str(epoch+1)+'/'+str(num_epochs))\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Train Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, total=len(val_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, total=len(test_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3g4HJ5cC-Ujr"
   },
   "source": [
    "## Yêu cầu 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zLg2QvdJx8JS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN2, self).__init__()\n",
    "        # self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1) # (32, 112, 112)\n",
    "        # self.bn1 = nn.BatchNorm2d(32)\n",
    "        # self.relu1 = nn.ReLU()\n",
    "\n",
    "        # self.dwconv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, groups=32) # (32, 112, 112)\n",
    "        # self.bn2 = nn.BatchNorm2d(32)\n",
    "        # self.relu2 = nn.ReLU()\n",
    "\n",
    "        # self.conv2 = nn.Conv2d(32, 64, kernel_size=1, stride=1, padding=0) # (64, 112, 112)\n",
    "        # self.bn3 = nn.BatchNorm2d(64)\n",
    "        # self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=1) # (64, 112, 112)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.dwconv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, groups=64) # (64, 56, 56)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding=0) # (128, 56, 56)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.dwconv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, groups=128) # (128, 56, 56)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0) # (128, 56, 56)\n",
    "        self.bn7 = nn.BatchNorm2d(128)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        self.dwconv4 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, groups=128) # (128, 28, 28)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        self.relu8 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0) # (256, 28, 28)\n",
    "        self.bn9 = nn.BatchNorm2d(256)\n",
    "        self.relu9 = nn.ReLU()\n",
    "\n",
    "        self.dwconv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, groups=256) # (256, 28, 28)\n",
    "        self.bn10 = nn.BatchNorm2d(256)\n",
    "        self.relu10 = nn.ReLU()\n",
    "\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0) # (256, 28, 28)\n",
    "        self.bn11 = nn.BatchNorm2d(256)\n",
    "        self.relu11 = nn.ReLU()\n",
    "\n",
    "        self.dwconv6 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, groups=256) # (256, 14, 14)\n",
    "        self.bn12 = nn.BatchNorm2d(256)\n",
    "        self.relu12 = nn.ReLU()\n",
    "\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=1, stride=1, padding=0) # (512, 14, 14)\n",
    "        self.bn13 = nn.BatchNorm2d(512)\n",
    "        self.relu13 = nn.ReLU()\n",
    "\n",
    "        # Repeat blocks for depthwise + pointwise convolutions 5 times\n",
    "        self.dwconv_repeats = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, groups=512),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(512, 512, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            for _ in range(5)\n",
    "        ])\n",
    "\n",
    "        self.dwconv12 = nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1, groups=512) # (512, 7, 7)\n",
    "        self.bn24 = nn.BatchNorm2d(512)\n",
    "        self.relu24 = nn.ReLU()\n",
    "\n",
    "        self.conv13 = nn.Conv2d(512, 1024, kernel_size=1, stride=1, padding=0) # (1024, 7, 7)\n",
    "        self.bn25 = nn.BatchNorm2d(1024)\n",
    "        self.relu25 = nn.ReLU()\n",
    "\n",
    "        self.dwconv13 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1, groups=1024) # (1024, 7, 7)\n",
    "        self.bn26 = nn.BatchNorm2d(1024)\n",
    "        self.relu26 = nn.ReLU()\n",
    "\n",
    "        self.conv14 = nn.Conv2d(1024, 1024, kernel_size=1, stride=1, padding=0) # (1024, 7, 7)\n",
    "        self.bn27 = nn.BatchNorm2d(1024)\n",
    "        self.relu27 = nn.ReLU()\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # (1024, 1, 1)\n",
    "        self.fc = nn.Linear(1024, num_classes) # (num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu4(self.bn4(self.dwconv2(x)))\n",
    "        x = self.relu5(self.bn5(self.conv3(x)))\n",
    "        x = self.relu6(self.bn6(self.dwconv3(x)))\n",
    "        x = self.relu7(self.bn7(self.conv4(x)))\n",
    "        x = self.relu8(self.bn8(self.dwconv4(x)))\n",
    "        x = self.relu9(self.bn9(self.conv5(x)))\n",
    "        x = self.relu10(self.bn10(self.dwconv5(x)))\n",
    "        x = self.relu11(self.bn11(self.conv6(x)))\n",
    "        x = self.relu12(self.bn12(self.dwconv6(x)))\n",
    "        x = self.relu13(self.bn13(self.conv7(x)))\n",
    "\n",
    "        x = self.dwconv_repeats(x)\n",
    "\n",
    "        x = self.relu24(self.bn24(self.dwconv12(x)))\n",
    "        x = self.relu25(self.bn25(self.conv13(x)))\n",
    "        x = self.relu26(self.bn26(self.dwconv13(x)))\n",
    "        x = self.relu27(self.bn27(self.conv14(x)))\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BhllHmFPBu7r"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels_indices = [np.argmax(label) for label in labels_one_hot]\n",
    "label_counts = Counter(labels_indices)\n",
    "class_weights = [1/label_counts[i] for i in range(len(lb.classes_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "NDhbFQKlFciN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = torch.tensor(class_weights).float().to(device)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        weighted_ce_loss = ce_loss * self.class_weights[targets]\n",
    "        return weighted_ce_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9eQxSFSFmuM",
    "outputId": "b9652f05-0bc6-4f98-83a1-09b0e0669498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:32<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.052841013206577886\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.05121300991897176\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.049841985197328936\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.04859779525275638\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.04699346509466811\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.04512571252700759\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.043196042363600036\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.04079662908504649\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.03849890675940892\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:31<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.036045529061882964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:06<00:00,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 29.55069124423963%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(lb.classes_)\n",
    "model = CNN2(num_classes).to(device)\n",
    "criterion = WeightedCrossEntropyLoss(class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch '+str(epoch+1)+'/'+str(num_epochs))\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, total=len(train_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, total=len(test_loader)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels_index = torch.max(labels.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels_index).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
